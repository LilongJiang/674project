% author: Man Cao, Lilong Jiang
\documentclass{article}
\usepackage[letterpaper]{geometry} \usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{relsize}
\usepackage{graphicx}

\newcommand{\code}[1]{\textsf{\smaller\verb~#1~}}

\begin{document}

\title{CSE5243 Assignment 4}
\author{Man Cao(cao.235), Lilong Jiang(jiang.573)}
\maketitle

\section{Work Separation}
Lilong mainly worked on hierarchical clustering. Man mainly worked on
K-means. In fact there were a lot of overlapping during the
work, we exchanged various ideas and wrote the this report together.
\section{Input}
After eliminating documents without topics, 11367 documents are left.\\
The input file has the following format:
\begin{verbatim}
{'NEWID':<value>, 'TOPICS':[value1, value2, ...], 'PLACES':[value1, value2, ...]}
{<term1>:<value1>, <term1>:<value2>, ...}
\end{verbatim}
Note that each document corresponds to two lines: the first line contains the
metadata of the document, the second line is the frequency vector.

\section{Metrics}
Cosine similarity and Jaccard similarity are used in this assignment.

\section{Algorithms}
\subsection{Hierarchical Clustering}
\subsubsection{Single Link}
We use single-link to cluster the documents.
\subsubsection{Implemtation Details}
Since the similarity between cluster1 and cluster2 is the same as the similarity between cluster2 and cluster1. We only need to store the upper triangle of the matrix.\\
Also considering the expensive time complexity of finding the min distance in the matrix, we transform the proximity matrix to a proximity list and sort the list before the clustering. In this way, we only check the distance from the last position. The mapping relationships between the documents to the clusters are record in a list and updated every time two clusters are merged.  
\subsection{K-means Clustering}
We implement the na\"ive K-means algorithm as described in the slides from
class:
\begin{enumerate}
  \item Randomly select K distinct documents as the initial centroids of K
  clusters.
  \item \label{repeat} For each document, compute similarities between the
  document and each of the K centroids; assign the document to the cluster whose
  centroid has largest similarity with the document.
  \item For each cluster, recompute its centroid, which is the mean of all
  documents in the cluster; also compute the distance between the new centroid
  and old centroid.
  \item If, for any cluster, the distance between the new centroid and old
  centroid is greater than thresold value, repeat from step~\ref{repeat}. 
\end{enumerate}

\noindent The distance between two documents d1 and d2 is simply $1.0 -
similarity(d1, d2)$. In our experiment, the threshold value is 0.001.

Interestingly, we observed that it is possible that K-means does not converge.
In one case, we ran K-means for nearly 40 hours with K=16 and cosine similarity,
and it did not give an output. Then we ran it again with the same configuration,
and it just finished in 35 minutes.

We found proof states that for large scale of data, it is quite possible for
K-means to oscillate between two or more partitions and never converge
\footnote{\url{http://www.clustan.com/k-means_critique.html\#FailureToConverge}}
\footnote{``Some methods for classification and analysis of multivariate
observations'', MacQueen, Berkeley Symposium, 1967, p. 288}. One solution is to
compute the variance of Sum of Squared Error (SSE) every time the algorithm
tries to move an element to a different cluster, considering the slight change
of the centroids of the two involved clusters. If such movement can reduce the
SSE, then move it; otherwise do not move the element. We did not implement this
approach, because it should be much slower, and we do not see the oscillation
case very often.
%This approach is to directly minimize the SSE for every step, thus can avoid
% the oscillation. However, it is definitely more expensive than the na\"ive K-means,
% and shares some similarity to the incremental updating centroids approach.

\section{Evaluation}
\subsection{Scalability}
The time for clustering 2, 4, 8, 16 and 32 clusters with Hierarchical Clustering
and K-means is show in Fig.
\subsection{Quality}
\subsubsection{Entropy}
For a document with multiple topics, each topic receives a \emph{vote} of
$1/n$, where $n$ is the number of topics in the document. The probability of a
topic $t$ within a cluster $C_x$ is then:
\begin{equation}
P(t|C_x)=\frac{\sum_{docs}v_{it}}{\sum_{docs}\sum_{topics}v_{ij}}
\end{equation}
where $v_{ij}$ is the vote of topic $j$ from document $i$.

In order to make sure the entropy falls within the range of $[0, 1.0]$, we use
$m$ as the base of the logarithm, where $m$ is the number of distinct topics in
the cluster. That is:
\begin{equation}
m = |C_x.topics|
\end{equation}

The entropy of the clustering is the sum of weighted entropy of each cluster. 

\subsubsection{Skew}
The skew is measured as variance of the cardinalities of different clusters.
\end{document}
