% author: Man Cao, Lilong Jiang
\documentclass{article}
\usepackage[letterpaper]{geometry} \usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{relsize}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{subcaption}

\newcommand{\code}[1]{\textsf{\smaller\verb~#1~}}

\begin{document}

\title{CSE5243 Assignment 5}
\author{Man Cao(cao.235), Lilong Jiang(jiang.573)}
\maketitle

\section{Work Separation}
Lilong mainly worked on part 1 of this project. Man mainly worked on
part 2. In fact there were a lot of overlapping during the
work, we exchanged various ideas and wrote the this report together.

\section{Software Used}
We use Orange \footnote{\url{http://orange.biolab.si}} to generate association
rules.
Orange implements the APRIORI algorithm to compute large itemsets and association
rules.

\section{Input}
After eliminating documents without topics, 11367 documents are left.\\
The input file has the following format:
\begin{verbatim}
{'NEWID':<value>, 'TOPICS':[value1, value2, ...], 'PLACES':[value1, value2, ...]}
{<term1>:<value1>, <term1>:<value2>, ...}
\end{verbatim}
Note that each document corresponds to two lines: the first line contains the
metadata of the document, the second line is the frequency vector.

\subsection{Data Transformation}
Orange supports transactional data input via the basket format. We first
transform our input to the basket format by dropping the <value> fields in
the input file, then use Orange to generate association rules.
We convert the topics in an document to uppercase, and keep the terms in
the feature vector in lowercase, in order to easily separate them for later
processing.

\section{Algorithms}
We refer the algorithm in part 1 of this project as Alg 1, and the
algorithm with clustering in part 2 as Alg 2. For the clustering algorithm in
part 2, we use K-means.

\subsection{Sorting Criteria}
After trying out several sorting criteria, we finally decided to sort the rules
by [lift, confidence, support]. That is to first order rules by lift; then for
rules with same lift, order them by confidence; finally order rules with same
lift and confidence by support.

The reason behind our choice is that there are a lot of rules with
high confidence and support that only predicts one dominant topic. First sorting
by lift can make rules that have good ability to predict rare topics
appear at the top, so that they get a better chance to be used.

\subsection{Subsumption}
We use the following algorithm to determine rule subsumption:

Rule r1 is subsumption by rule r2 if all of the following conditions are
satisfied:
\begin{enumerate}
  \item r2 comes before r1 in the list of sorted rules.
  \item r1 can not predict any new topics compared to r2. That is: r1.right
  $\subseteq$ r2.right.
  \item all data instances that match r1 also matched r2. That is:
  r1.match\_both $\subseteq$ r2.match\_both.
\end{enumerate}

\subsection{Handling Data Skew}

\subsection{Accuracy for Multiple Topics}


\section{Parameter Values}
\subsection{Minimum Support}
\subsection{K-most confident rules}
\subsection{Number of clusters}

\input{eval}

\end{document}
